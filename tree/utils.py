# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dmf5q4xP9Ypnu7VhNrvLcuAmmQskopLz
"""

import pandas as pd
import numpy as np
import math
from scipy.special import xlogy

def check_ifreal(y: pd.Series) -> bool:
    return y.dtype in [np.float64, np.float32, np.int64, np.int32]

def entropy(Y: pd.Series) -> float:
    # Calculate the probability of each unique value in Y
    probabilitiy = Y.value_counts()/Y.value_counts.sum()

    # Calculate entropy using the formula
    entropy_value = -np.sumsum(probabilitiy*np.log2(probabilitiy)) ##as entropy = -sigma(p*log2(p))

    return entropy_value

def gini_index(Y: pd.Series) -> float:
  ## gini index is 1 - sig(p^2)
  probabilities = Y.value_counts()/Y.value_counts.sum()
  gini = 1 - np.sum(probabilities**2)
  return gini

def information_gain(Y: pd.Series, attr: pd.Series) -> float:
    """
    Function to calculate the information gain
    """
    # Calculate the entropy before splitting
    entropy_before = entropy(Y)

    # Calculate weighted entropy after splitting by the attribute
    unique_values, counts = np.unique(attr, return_counts=True)
    weighted_entropy_after = sum((counts[i] / len(Y)) * entropy(Y[attr == unique_values[i]]) for i in range(len(unique_values)))

    # Calculate information gain
    information_gain_value = entropy_before - weighted_entropy_after

    return information_gain_value

def opt_split_attribute(X: pd.DataFrame, y: pd.Series, criterion, features: pd.Series):
    best_attribute = None
    best_value = None
    best_score = float('inf')  # Initialize with a large value for gini, small for mse or entropy

    for attribute in features:
        # Iterate over each attribute and find the best split value
        values = X[attribute].unique()

        for value in values:
            # Split the data
            X_left, y_left, X_right, y_right = split_data(X, y, attribute, value)

            # Calculate the score based on the criterion
            score = None
            if criterion == 'gini':
                score = gini_index(y)
            if criterion == 'entropy':
                score = entropy(y_left) + entropy(y_right)
            # Update the best split if the current one is better

            if score < best_score:
                best_score = score
                best_attribute = attribute
                best_value = value

    return best_attribute, best_value

def split_data(X: pd.DataFrame, y: pd.Series, attribute, value):
  if pd.api.types.is_numeric_dtype(X[attribute]): #checks whether given dtype is numeric
    # Real-valued feature
    mask = X[attribute] <= value
  else:
    # Discrete feature
    mask = X[attribute] == value
    X_left, y_left = X[mask], y[mask]
    X_right, y_right = X[~mask], y[~mask]

    return X_left, y_left, X_right, y_right




